# 第一章 机器学习领域

## 什么是机器学习

## 为什么使用机器学习？

## 机器学习系统的类型

### supervised/unsupervised learning

supervised learning

最常见的supervised learning任务是回归（预测值）和分类（预测类）。

最重要的supervised learning算法：

-k-最近邻

-线性回归

-逻辑回归

-支持向量机SVMs

-决策树和随机森林

-神经网络（部分是非监督学习）

unsupervised learning

最重要的unsupervised learning算法（大部分在第八章和第九章）

-聚类：k均值、DBSCAN、层次聚类分析（HCA）

-异常检测和新颖性检测：一级支持向量机、隔离林

-可视化和降维：主成分分析（PCA）、核心主成分分析、局部线性嵌入（LLE）、t分布随机邻域嵌入（t-SNE）

-关联规则学习：先验、怡亨

### 批量和在线学习

### 基于实例与基于模型的学习

## 机器学习的主要挑战

### 不足量的训练数据

### 没有代表性的训练数据

### 低质量的数据

### 不相关的特征

### 过拟合的训练数据

### 欠拟合的训练数据

### 落后

## 实验和废除

### 超参数调整和模式选择

超参数hypoparameter：

参数：

情景：

交叉验证cross validation

### 数据错配



# 第二章 端对端机器学习项目

## Machine Learning项目清单

### 1、框定问题，放眼大局

#### 选择绩效指标

回归问题的一个典型的性能测量是均方根误差RMSE。

如果有很多异常值，则需要使用绝对误差MAE。

RMSE和MAE是测量两个变量之间的距离：预测变量和目标值变量之间的距离。

#### 检测假设

### 2、得到数据

#### 下载数据

#### 观察数据结构

#### 创立测试系列

### 3、探究数据，并将数据可视化以洞察数据

1）创建数据copy来做研究（做一个可使用的大小）

2）创建一个Jupyter notebook来记录你的数据探索

3）学习每个属性和特征：名称、类型（整数/浮点数，等）、确实值占比、干扰值及其类型，分布类型（高斯、制服、对数等）

4）如果是supervised learning任务，识别目标属性

5）数据可视化

6）学习属性之间的相关性

### 4、准备数据以更好地将底层数据模式暴露给机器学习算法

在数据副本上工作（完好无损地保存原数据）。写下你使用的所有的数据转换函数

1）清洁数据

### 5、探究更多不同模式，选最佳的模型并训练它

6、微调你的模式并且结合成一个更好的解决方案

7、展示你的方案

8、启动、监控、保持你的系统

### 6、微调模型

### 7、展示解决方案

### 8、启动、监控和维护您的系统



# 第三章 分类

## 训练二元分类器

## 性能测量

### 使用交叉测试测量精确性

### 混淆矩阵

### 精确与召回=TPR（True Positive Rate）=TP/（TP+FN）=TP/P

Precision=TP/（TP+FP）

TPR/Recall是我们预测对了的正确的类型

FPR=FP/N=FP/（FP+TN）

N=所有的错误观察对象

FPR是实际上错误的类型，但是被预测成正确的

F1分数

### 精确/召回 权衡

精确/召回权衡意思是高精确会导致低召回。

![image](https://user-images.githubusercontent.com/121781739/223398048-d62e3f60-4272-4e62-ab36-24d75728cf66.png)


### ROC Curve（Receiver Operating Characteristic Curve，受试者工作特征曲线）

AUC（Area Under Curve）

经验法则：如果正确的分类是稀有的，或者你

一对一和一对剩余

二元分类器通过一对剩余或者一对一策略，可以给多个分类问题使用。


## 多类分类

 一对一

 一对剩余

### 错误分析

### 多标签分类

本课程不涉及

### 多输出分类



# 第四章 训练模型

## 线性回归

是一种模型

### 正规方程

向量

矩阵

方程式

### 计算的复杂性


## 梯度下降

梯度下降是一种常用的优化算法，用于求解函数的最小值。

梯度下降通常分为批量梯度下降（Batch Gradient Descent）、随机梯度下降（Stochastic Gradient Descent）和小批量梯度下降（Mini-batch Gradient Descent）三种形式。

批量梯度下降每次使用所有的样本计算损失函数和梯度，计算速度较慢，但是收敛稳定；随机梯度下降每次使用一个样本计算损失函数和梯度，计算速度较快，但是收敛速度不稳定；小批量梯度下降使用一部分样本计算损失函数和梯度，权衡了计算速度和收敛稳定性。

局部最小值（Local Minimum）是一个函数在某个局部区间内取得的最小值，但不一定是全局最小值。在优化问题中，目标是找到函数的全局最小值，但在实际求解过程中，通常需要通过优化算法来找到函数的局部最小值，并希望它接近于全局最小值。

在梯度下降算法中，由于每次更新参数时只考虑当前位置的梯度信息，因此容易被困在局部最小值中，而无法跳出局部最小值去寻找全局最小值。因此，为了避免梯度下降算法陷入局部最小值，通常需要进行多次随机初始化和迭代，从不同的起点出发，尝试不同的参数组合，以期最终找到全局最小值。

### 批量梯度下降 Batch 

### 随机梯度下降 Stochastic

scikit learn使用该方法，因为更快

### 小批量梯度下降 Mini-batch


## 多项式回归polynomregression

有时数据无法使用线性回归被建模，我们就需要多项式回归来帮忙。

在scikit-learn中建立多项式变量（新特征）通过PolynomalFeatures。



## 学习曲线

偏差方差均衡

一个模式的‘generalization error’可以被分成三部分：

1、偏差：由于模型指定错误而导致出现偏差，比如我们使用线性回归在一个正方形上。

2、方差：由于模型敏感

3、不可还原的错误：

增加模型复杂度会增加方差但是减少偏差，反之亦然

所以一个复杂的模型不一定会更好：线性回归可以比多项式回归更好

## 正则化线性模型

来正则一个模型意味着限制它。如果我们限制一个模型我们会增加偏差但是方差会减少

### 岭回归

岭回归（Ridge Regression）是一种线性回归的扩展形式，它通过在损失函数中添加一个L2正则化项来避免过度拟合的问题。

我们想要损失函数尽量小

通过gridsearch来选择超参数

### 套索回归

Lasso回归（Least Absolute Shrinkage and Selection Operator Regression）是一种线性回归的扩展形式，通过在损失函数中添加一个L1正则化项来避免过度拟合的问题。

lasso尝试尽量减少参数betta到0.

阿尔法决定我们的惩罚力度

### 弹性网 Elastic Net

岭回归和Lasso回归的结合

### 早停


## 逻辑回归

逻辑回归其实应该被称为逻辑分类，因为我们处理的是分类的问题，而不是回归的问题。

### 估计可能性

逻辑回归是一个二项式分类，可以给一个观察对象估计概率。概率在0和1之间。如果概率大于0.5， 则是ja；如果概率小于0.5，则是nej。

### 训练和成本功能

### 决定边界

### 多项式回归






# 第五章 支持向量机

支持向量机（Support Vector Machines，简称 SVM）是一种广泛应用于分类和回归问题的监督学习算法。SVM 的核心思想是找到一个决策边界，使得两个不同类别之间的边界间隔最大化。这个决策边界称为最大间隔超平面。

SVM适合中小型数据。

支持向量机对数据的缩放非常敏感，所以我们一定要标准化数据，比如使用Scikit-learn中的StandardScaler

## 线性支持向量机分类

线性支持向量机（Linear SVM）分类是一种基于支持向量机（SVM）的监督学习方法，用于解决二分类问题。线性 SVM 的目标是找到一个最优的线性决策边界（称为超平面），使得两个类别之间的间隔最大化。这个最大间隔超平面可以很好地泛化到新的数据，从而提高分类性能。

在线性SVM分类的情况下，用于分离数据点的超平面是一条直线。该算法的工作方式是找到最优系数（也称为权重）和超平面的截距，以最大化两个类别之间的间隔。

要训练线性SVM分类器，算法将一组标记的训练数据点作为输入。目标是找到最优超平面，同时最大化间隔并将分类错误最小化，从而将两个类别分开。

大边距分类：这是线性支持向量机分类的基本理念，就是我们尝试建立一个宽‘路’在每个分类之间。

硬边距分类：我们要求每一方都没有错误的观察对象。如果有异常值，则无法使用该方法。硬边距的毛病可以用软边距分类来解决。

### 软边距分类

软间隔分类是一种用于解决线性不可分数据的方法，它通过允许一些数据点处于分类边界之内，来放宽对分类器的约束条件。软间隔分类是线性支持向量机（SVM）的一种扩展形式。

在硬间隔分类中，SVM算法要求所有数据点都必须被正确分类并且能够完美地被一个超平面分隔开。然而，在实际情况中，一些数据点可能会出现噪声或者离群点的情况，这使得所有的数据点都无法被完美地分类。在这种情况下，软间隔分类允许一些数据点在分类边界之内，以此来实现更好的分类效果。

为了实现软间隔分类，SVM算法会引入一个松弛变量，用于测量每个数据点与分类边界之间的距离。松弛变量允许一些数据点被错误地分类，并且可以在最大化间隔的同时，找到一个最小化错误分类和松弛变量的平衡点。

软间隔分类的目标是找到一个最优的超平面（最宽的‘路’），该超平面可以将大多数数据点正确地分为它们的类别，并且在最小化松弛变量和错误分类的同时，也可以使间隔最大化。

在实际情况中，软间隔分类可以处理更复杂的数据集，因为它可以容忍一些数据点的噪声和离群点，并且可以在更具鲁棒性的情况下完成分类任务。

软边距分类的

## 非线性支持向量机分类

非线性支持向量机（SVM）分类是一种用于分类非线性数据的算法。当数据不是线性可分的时候，线性SVM分类算法无法很好地处理数据，而非线性SVM分类算法则可以使用核函数将数据映射到高维空间，从而将非线性问题转换为线性问题。

比如可以使用PolynomialFeatures之后数据变为线性可分。

### 多项式核Polynom Kernel

多项式核是一种常用的核函数，用于将低维空间中的数据点映射到高维空间中进行非线性分类。

给数据集增加数据（将原数据平方）会让模型变慢，我们可以使用‘核把戏’来

coef0是一个超参数用来控制高阶多项式项应该影响多少(这个超参数仅在kernel=‘poly’或者‘sigmoid'时才相关）。 在实际情况中，我们使用GridSearch来选择coef0来给最好的预测。

### 相似性特征

相似性特征（similarity features）是一种将输入数据表示为基于相似性的特征向量的方法。这种方法将每个数据点表示为与一组参考点之间的相似性度量值，这些参考点可以是训练数据中的数据点或者是通过其他方法选择的点。

在使用相似性特征时，首先需要定义一个相似性度量函数，该函数用于计算输入数据点和参考点之间的相似性程度。

然后，对于每个数据点，使用相似性度量函数计算它与每个参考点之间的相似性度量值。这些相似性度量值可以被组合成一个特征向量，该向量表示数据点与每个参考点之间的相似性度量值。在训练分类器时，这个特征向量可以被用作输入数据的新表示形式。

相似性特征的优点在于它可以很好地处理非线性分类问题，因为相似性度量函数可以是任意的函数形式。相似性特征的缺点在于它通常需要计算大量的相似性度量值，这会导致计算成本较高，并且需要更多的存储空间来存储特征向量。此外，在选择参考点时，也需要考虑如何选择最具代表性的参考点来表示数据点的相似性度量值。

超参数gamma是 

### 高斯径向基函数（Gaussian radial basis function，RBF）核

是一种常用的核函数，用于将低维空间中的数据点映射到高维空间中进行非线性分类。使用高斯径向基函数的支持向量机可以处理一些非线性分类问题，尤其是当数据点呈现出较为复杂的分布模式时。高斯径向基函数是一种通用的核函数，它在支持向量机中得到了广泛的应用，并且在其他机器学习算法中也被广泛使用。

### 计算复杂度（Computational complexity）

是指一个算法在计算机上执行时所需的资源量，通常用时间复杂度和空间复杂度来衡量。

时间复杂度是指算法执行所需的时间，通常用算法中基本操作的执行次数来表示。

空间复杂度是指算法在执行过程中所需的存储空间，通常用算法中所使用的额外空间量来表示。

在算法设计中，我们通常希望时间复杂度和空间复杂度尽量小，以提高算法的效率和速度。然而，时间复杂度和空间复杂度通常是相互矛盾的，一个算法的时间复杂度可以很低，但空间复杂度可能很高，反之亦然。

因此，在算法设计中，我们需要权衡时间复杂度和空间复杂度，并根据具体应用场景的要求选择最优的算法。对于一些计算复杂度较高的算法，我们也可以通过优化算法或者使用并行计算等方式来提高其计算效率。

## 支持向量机回归

支持向量机（SVM）回归是一种用于回归任务的算法，它与SVM分类相似，但不同之处在于它试图拟合一个连续的函数，而不是离散的分类。在SVM回归中，算法的目标是最小化数据点与回归函数之间的误差，并找到一个最优的回归函数。

与SVM分类类似，SVM回归也有两个重要的超参数：正则化参数C和核函数。正则化参数C控制了回归函数与数据点之间的权衡，较大的C可以使回归函数更好地拟合数据点，但可能会导致过拟合。核函数的选择也会影响回归函数的性能，常用的核函数包括径向基函数和多项式核函数等。



## 

# 第六章 决策树

## 决策树的训练和可视化

## 预测

## 预估分类概率

## CART（Classification and Regression Trees）训练术语

## 计算复杂性

## 基尼杂质或熵

基尼杂质（Gini impurity）和熵（entropy）是用于衡量决策树划分质量的指标。

基尼杂质衡量的是在样本集合中随机选取两个样本，其类别不一致的概率。如果一个集合中所有样本都属于同一类别，那么该集合的基尼杂质为0；如果样本均匀分布在各个类别中，那么该集合的基尼杂质最大。

熵是信息论中的概念，用于衡量信息的不确定性，表示样本集合中信息的混乱程度。当一个集合中所有样本都属于同一类别时，熵为0；当样本均匀分布在各个类别中时，熵最大。

基尼杂质和熵都可以用于决策树的分类问题中，二者的效果类似，但在不同的数据集上，可能会有所不同。一般来说，当类别较少时，使用基尼杂质更合适；当类别较多时，使用熵更合适。

## 正则化超参数

## 回归

## 不稳定



# 第七章 综合学习和随机森林

# 第八章 降维

