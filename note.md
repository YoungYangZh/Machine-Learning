# 第一章 机器学习领域

## 机器学习系统的类型

### supervised/unsupervised learning

#### supervised learning

最常见的supervised learning任务是回归（预测值）和分类（预测类）。

最重要的supervised learning算法：

-k-最近邻

-线性回归

-逻辑回归

-支持向量机SVMs

-决策树和随机森林

-神经网络（部分是非监督学习）

#### unsupervised learning

最重要的unsupervised learning算法（大部分在第八章和第九章）

-聚类：k均值、DBSCAN、层次聚类分析（HCA）

-异常检测和新颖性检测：一级支持向量机、隔离林

-可视化和降维：主成分分析（PCA）、核心主成分分析、局部线性嵌入（LLE）、t分布随机邻域嵌入（t-SNE）

-关联规则学习：先验、怡亨




超参数调整和模型选择

超参数hypoparameter：

参数：

情景：

交叉验证cross validation



# 第二章 端对端机器学习项目

项目步骤

## 1、看大图

### 框定问题

### 选择绩效指标

回归问题的一个典型的性能测量是均方根误差RMSE。

如果有很多异常值，则需要使用绝对误差MAE。

RMSE和MAE是测量两个变量之间的距离：预测变量和目标值变量之间的距离。

### 检测假设

## 2、得到数据

## 3、发现数据并将数据可视化以洞察数据

## 4、为machine learning算法准备数据

## 5、选一个模型并训练它

## 6、微调模型

## 7、展示解决方案

## 8、启动、监控和维护您的系统



# 第三章 分类

## 训练二元分类器

## 性能测量

### 使用交叉测试测量精确性

### 混淆矩阵

### 精确与召回=TPR（True Positive Rate）=TP/（TP+FN）=TP/P

Precision=TP/（TP+FP）

TPR/Recall是我们预测对了的正确的类型

FPR=FP/N=FP/（FP+TN）

N=所有的错误观察对象

FPR是实际上错误的类型，但是被预测成正确的

F1分数

### 精确/召回 权衡

精确/召回权衡意思是高精确会导致低召回。

![image](https://user-images.githubusercontent.com/121781739/223398048-d62e3f60-4272-4e62-ab36-24d75728cf66.png)


### ROC Curve（Receiver Operating Characteristic Curve，受试者工作特征曲线）

AUC（Area Under Curve）

经验法则：如果正确的分类是稀有的，或者你

一对一和一对剩余

二元分类器通过一对剩余或者一对一策略，可以给多个分类问题使用。


## 多类分类

 一对一

 一对剩余

### 错误分析

### 多标签分类

本课程不涉及

### 多输出分类



# 第四章 训练模型

## 线性回归

是一种模型

### 正规方程

向量

矩阵

方程式

### 计算的复杂性


## 梯度下降

梯度下降是一种常用的优化算法，用于求解函数的最小值。

梯度下降通常分为批量梯度下降（Batch Gradient Descent）、随机梯度下降（Stochastic Gradient Descent）和小批量梯度下降（Mini-batch Gradient Descent）三种形式。

批量梯度下降每次使用所有的样本计算损失函数和梯度，计算速度较慢，但是收敛稳定；随机梯度下降每次使用一个样本计算损失函数和梯度，计算速度较快，但是收敛速度不稳定；小批量梯度下降使用一部分样本计算损失函数和梯度，权衡了计算速度和收敛稳定性。

局部最小值（Local Minimum）是一个函数在某个局部区间内取得的最小值，但不一定是全局最小值。在优化问题中，目标是找到函数的全局最小值，但在实际求解过程中，通常需要通过优化算法来找到函数的局部最小值，并希望它接近于全局最小值。

在梯度下降算法中，由于每次更新参数时只考虑当前位置的梯度信息，因此容易被困在局部最小值中，而无法跳出局部最小值去寻找全局最小值。因此，为了避免梯度下降算法陷入局部最小值，通常需要进行多次随机初始化和迭代，从不同的起点出发，尝试不同的参数组合，以期最终找到全局最小值。

### 批量梯度下降 Batch 

### 随机梯度下降 Stochastic

scikit learn使用该方法，因为更快

### 小批量梯度下降 Mini-batch


## 多项式回归polynomregression

有时数据无法使用线性回归被建模，我们就需要多项式回归来帮忙。

在scikit-learn中建立多项式变量（新特征）通过PolynomalFeatures。



## 学习曲线

偏差方差均衡

一个模式的‘generalization error’可以被分成三部分：

1、偏差：由于模型指定错误而导致出现偏差，比如我们使用线性回归在一个正方形上。

2、方差：由于模型敏感

3、不可还原的错误：

增加模型复杂度会增加方差但是减少偏差，反之亦然

所以一个复杂的模型不一定会更好：线性回归可以比多项式回归更好

## 正则化线性模型

来正则一个模型意味着限制它。如果我们限制一个模型我们会增加偏差但是方差会减少

### 岭回归

岭回归（Ridge Regression）是一种线性回归的扩展形式，它通过在损失函数中添加一个L2正则化项来避免过度拟合的问题。

我们想要损失函数尽量小

通过gridsearch来选择超参数

### 套索回归

Lasso回归（Least Absolute Shrinkage and Selection Operator Regression）是一种线性回归的扩展形式，通过在损失函数中添加一个L1正则化项来避免过度拟合的问题。

lasso尝试尽量减少参数betta到0.

### 弹性网 Elastic Net

岭回归和Lasso回归的结合

### 早停


## 逻辑回归



### 估计可能性

### 训练和成本功能

### 决定边界

### 多项式回归






# 第五章 支持向量机

# 第六章 决策树

# 第七章 综合学习和随机森林

# 第八章 降维

# Machine Learning项目清单

## 1、框定问题，放眼大局

## 2、得到数据

## 3、探究数据，得到洞察

1）创建数据copy来做研究（做一个可使用的大小）

2）创建一个Jupyter notebook来记录你的数据探索

3）学习每个属性和特征：名称、类型（整数/浮点数，等）、确实值占比、干扰值及其类型，分布类型（高斯、制服、对数等）

4）如果是supervised learning任务，识别目标属性

5）数据可视化

6）学习属性之间的相关性

## 4、准备数据以更好地将底层数据模式暴露给机器学习算法

在数据副本上工作（完好无损地保存原数据）。写下你使用的所有的数据转换函数

1）清洁数据

## 5、探究更多不同模式，选最佳的

6、微调你的模式并且结合成一个更好的解决方案

7、展示你的方案

8、启动、监控、保持你的系统
